{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers bertopic umap-learn hdbscan numpy python_dotenv azure-storage-blob panda pytz ipywidgets python-dotenv sendgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import json, os, pytz, html\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "import json\n",
    "from datetime import datetime, timedelta, UTC\n",
    "from sendgrid import SendGridAPIClient\n",
    "from sendgrid.helpers.mail import Mail\n",
    "from dotenv import load_dotenv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(content):\n",
    "    message = Mail(\n",
    "        from_email=os.getenv(\"MAIL_FROM\"),\n",
    "        to_emails=os.getenv(\"MAIL_TO\"),\n",
    "        subject= \"Weekly Chatbot Summary\",\n",
    "        html_content=content)\n",
    "\n",
    "    try:\n",
    "        sg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))\n",
    "        response = sg.send(message)\n",
    "        print(response.status_code)\n",
    "        print(response.body)\n",
    "        print(response.headers)\n",
    "    except Exception as e:\n",
    "        print(e.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/content/drive/MyDrive/Colab Notebooks/LCI Chatbot/.env')\n",
    "AZURE_CONN_STR = os.environ[\"AZURE_CSV_CONTAINER\"]\n",
    "print(AZURE_CONN_STR)\n",
    "\n",
    "AZURE_CONN_STR = os.environ[\"AZURE_CSV_CONTAINER\"]\n",
    "DATA_CONTAINER = \"chat-logs\"\n",
    "DATA_BLOB_NAME = \"logs\"\n",
    "\n",
    "SUMMARY_CONTAINER = \"weekly-summaries\"\n",
    "SUMMARY_BLOB_NAME = \"topic_discovery.txt\"\n",
    "\n",
    "MODEL_CONTAINER = \"models\"\n",
    "MODEL_BLOB_NAME = \"topic_inference.zip\"\n",
    "\n",
    "MODEL_DIR = \"./topic_model_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda38869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_week_window(tz_name=\"America/Chicago\", which=\"last_full_iso_week\", offset_weeks=0, now_utc=None):\n",
    "    tz = pytz.timezone(tz_name)\n",
    "    if now_utc is None:\n",
    "        now_utc = datetime.now(UTC)\n",
    "\n",
    "    now_local = now_utc.astimezone(tz)\n",
    "    dow = now_local.isoweekday()\n",
    "    start_of_this_week = (now_local - timedelta(days=dow - 1)).replace(\n",
    "        hour=0, minute=0, second=0, microsecond=0\n",
    "    )\n",
    "    end = start_of_this_week\n",
    "    start = end - timedelta(weeks=1)\n",
    "\n",
    "    if offset_weeks:\n",
    "        end -= timedelta(weeks=offset_weeks)\n",
    "        start -= timedelta(weeks=offset_weeks)\n",
    "\n",
    "    return start, end\n",
    "\n",
    "def iso_week_id_from_start(start_dt):\n",
    "    iso = start_dt.isocalendar()\n",
    "    return f\"{iso.year}-W{iso.week:02d}\"\n",
    "\n",
    "def append_summary_to_azure(conn_str, container, blob_name, text_block):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "    try:\n",
    "        blob_service_client.create_container(container)\n",
    "    except ResourceExistsError:\n",
    "        pass\n",
    "    append_client = blob_service_client.get_blob_client(container, blob_name)\n",
    "    if not append_client.exists():\n",
    "        append_client.create_append_blob()\n",
    "\n",
    "    payload = (text_block.rstrip() + \"\\n\\n\" + \"-\"*60 + \"\\n\\n\").encode(\"utf-8\")\n",
    "    append_client.append_block(payload)\n",
    "    print(f\"Appended text to {container}/{blob_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b29f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_summary(conn_str, container, blob_name, local_path=\"./summary.txt\"):\n",
    "\n",
    "    service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "    client = service_client.get_blob_client(container, blob_name)\n",
    "\n",
    "    os.makedirs(os.path.dirname(local_path) or \".\", exist_ok=True)\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        data = client.download_blob()\n",
    "        f.write(data.readall())\n",
    "\n",
    "    print(f\"Text file saved at {local_path}\")\n",
    "\n",
    "def download_and_unzip_model(conn_str, container, blob_name, out_dir):\n",
    "\n",
    "    service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "    client = service_client.get_blob_client(container, blob_name)\n",
    "    if not client.exists():\n",
    "        print(f\"No remote model found at {container}/{blob_name}\")\n",
    "        return False\n",
    "\n",
    "    import zipfile, io\n",
    "    data = client.download_blob().readall()\n",
    "    with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    print(f\"Model extracted to {out_dir}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discovery(container, in_blob, start=None, end=None, tz_name = \"America/Chicago\"):\n",
    "if not os.path.exists(in_blob):\n",
    "    print(\"Data not found locally, downloading data...\")\n",
    "    client = BlobServiceClient.from_connection_string(AZURE_CONN_STR)\n",
    "    blob = client.get_blob_client(container, in_blob)\n",
    "\n",
    "    with open(in_blob, \"wb\") as f:\n",
    "        downloader = blob.download_blob()\n",
    "        for chunk in downloader.chunks():\n",
    "            f.write(chunk)\n",
    "\n",
    "\n",
    "df = pd.read_json(in_blob, lines=True)\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[\"time\", \"question\"])\n",
    "df[\"question\"] = (df[\"question\"]\n",
    "                  .str.lower()\n",
    "                  .str.replace(r\"https?://\\S+\",\" \", regex=True)\n",
    "                  .str.replace(r\"\\s+\",\" \", regex=True)\n",
    "                  .str.strip()\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tz = pytz.timezone(tz_name)\n",
    "df[\"time_local\"] = df[\"time\"].dt.tz_convert(tz)\n",
    "if start is None or end is None:\n",
    "    start, end = compute_week_window(tz_name=tz_name, which=\"last_full_iso_week\")\n",
    "print(f\"Summarizing window: {start.isoformat()} to {end.isoformat()} [{tz_name}]\")\n",
    "\n",
    "week_mask = (df[\"time_local\"] >= start) & (df[\"time_local\"] < end)\n",
    "week_df = df.loc[week_mask].copy()\n",
    "partial_df = week_df.sample(frac=1, random_state=42) # shuffles\n",
    "\n",
    "if download_and_unzip_model(AZURE_CONN_STR, MODEL_CONTAINER, MODEL_BLOB_NAME, MODEL_DIR):\n",
    "    print(\"Loading existing models...\")\n",
    "\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    topic_model = BERTopic.load(f\"{MODEL_DIR}/topic_model\", embedding_model=embed_model)\n",
    "    topics, _ = topic_model.transform(partial_df[\"question\"].tolist())\n",
    "else:\n",
    "    print(\"Failed to download model\")\n",
    "    return 0\n",
    "\n",
    "partial_df[\"topic_id\"] = topics\n",
    "topic_dict = topic_model.get_topics()\n",
    "\n",
    "\n",
    "\n",
    "def auto_title(tid, topn=3):\n",
    "    if tid == -1:\n",
    "        return \"Other / Noise\"\n",
    "    words = [w for w, _ in topic_dict[tid][:topn]]\n",
    "    return \" / \".join([w.capitalize() for w in words]) if words else \"Unknown\"\n",
    "\n",
    "summary = (\n",
    "        partial_df[\"topic_id\"]\n",
    "        .value_counts()\n",
    "        .rename_axis(\"topic\")\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "if summary.empty:\n",
    "    iso_week = iso_week_id_from_start(start)\n",
    "    summary_dict = {\n",
    "        \"run_id\": iso_week,\n",
    "        \"week_start\": start.date().isoformat(),\n",
    "        \"week_end\": end.date().isoformat(),\n",
    "        \"n_prompts\": 0,\n",
    "        \"top_topics\": [],\n",
    "        \"method\": {\"algo\": \"BERTopic\", \"embedding\": \"all-MiniLM-L6-v2\", \"probabilities\": False},\n",
    "        \"notes\": \"No data in window\",\n",
    "    }\n",
    "    return summary, iso_week, summary_dict, partial_df, start, end\n",
    "\n",
    "summary[\"topic_name\"] = summary[\"topic\"].apply(auto_title)\n",
    "summary[\"share\"] = (summary[\"count\"] / max(len(partial_df), 1)).round(3)\n",
    "\n",
    "top_topics = [\n",
    "    {\n",
    "        \"topic_id\": int(row[\"topic\"]),\n",
    "        \"topic_name\": str(row[\"topic_name\"]),\n",
    "        \"count\": int(row[\"count\"]),\n",
    "        \"share\": float(row[\"share\"]),\n",
    "    }\n",
    "    for _, row in summary.iterrows()\n",
    "]\n",
    "\n",
    "iso_week = iso_week_id_from_start(start)\n",
    "\n",
    "summary_dict = {\n",
    "    \"run_id\": iso_week,\n",
    "    \"week_start\": start.date().isoformat(),\n",
    "    \"week_end\": end.date().isoformat(),\n",
    "    \"n_prompts\": int(len(partial_df)),\n",
    "    \"top_topics\": top_topics,\n",
    "    \"method\": {\"algo\": \"BERTopic\", \"embedding\": \"all-MiniLM-L6-v2\", \"probabilities\": False},\n",
    "    \"notes\": None,\n",
    "}\n",
    "\n",
    "print(\"Summary JSON preview:\", json.dumps(summary_dict)[:300] + \"...\")\n",
    "return summary, iso_week, summary_dict, partial_df, start, end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510821ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plain_text_summary(iso_week, start, end, window_df, summary_df, top_k=10):\n",
    "total_msgs = int(len(window_df))\n",
    "flagged = int(window_df.get(\"flagged\", pd.Series(dtype=bool)).sum()) if total_msgs else 0\n",
    "\n",
    "lines = []\n",
    "lines.append(f\"Weekly Topic Summary — {iso_week}\")\n",
    "lines.append(f\"Window: {start.date().isoformat()} to {end.date().isoformat()}\")\n",
    "lines.append(f\"Total messages: {total_msgs}\")\n",
    "lines.append(f\"Flagged messages: {flagged}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "if total_msgs == 0 or summary_df.empty:\n",
    "    lines.append(\"No messages this week.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "lines.append(\"Top topics this week:\")\n",
    "\n",
    "for _, row in summary_df.head(top_k).iterrows():\n",
    "    pct = f\"{row['share']*100:.1f}%\"\n",
    "    lines.append(f\"  • {row['topic_name']} — {pct} ({int(row['count'])} messages)\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_html_summary(iso_week, start, end, window_df, summary_df, top_k=10):\n",
    "total_msgs = int(len(window_df))\n",
    "flagged = int(window_df.get(\"flagged\", pd.Series(dtype=bool)).sum()) if total_msgs else 0\n",
    "\n",
    "#header\n",
    "header = (\n",
    "    f\"<h2 style='margin:0 0 8px;'>Weekly Topic Summary — {html.escape(str(iso_week))}</h2>\"\n",
    "    f\"<p style='margin:0 0 12px; line-height:1.5;'>\"\n",
    "    f\"<strong>Window:</strong> {start.date().isoformat()} to {end.date().isoformat()}<br>\"\n",
    "    f\"<strong>Total messages:</strong> {total_msgs}<br>\"\n",
    "    f\"<strong>Flagged messages:</strong> {flagged}\"\n",
    "    f\"</p>\"\n",
    ")\n",
    "\n",
    "if total_msgs == 0 or summary_df is None or summary_df.empty:\n",
    "    body = \"<p>No messages this week.</p>\"\n",
    "    return f\"<!doctype html><html><body>{header}{body}</body></html>\"\n",
    "\n",
    "items = []\n",
    "for _, row in summary_df.head(top_k).iterrows():\n",
    "    topic = html.escape(str(row[\"topic_name\"]))\n",
    "    pct = f\"{row['share']*100:.1f}%\"\n",
    "    cnt = int(row[\"count\"])\n",
    "    items.append(f\"<li>{topic} — {pct} ({cnt} messages)</li>\")\n",
    "\n",
    "body = (\n",
    "    \"<h3 style='margin:16px 0 8px;'>Top topics this week:</h3>\"\n",
    "    f\"<ul style='margin:8px 0 0 20px; padding:0;'>{''.join(items)}</ul>\"\n",
    ")\n",
    "return f\"<!doctype html><html><body style='font-family:Arial,Helvetica,sans-serif; font-size:14px; color:#111;'>{header}{body}</body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "  os.mkdir(MODEL_DIR)\n",
    "\n",
    "summary_df, iso_week, summary_dict, window_df, start, end = discovery(DATA_CONTAINER, DATA_BLOB_NAME)\n",
    "\n",
    "readable_text = build_plain_text_summary(\n",
    "  iso_week=iso_week,\n",
    "  start=start,\n",
    "  end=end,\n",
    "  window_df=window_df,\n",
    "  summary_df=summary_df,\n",
    "  top_k=10\n",
    ")\n",
    "\n",
    "append_summary_to_azure(AZURE_CONN_STR, SUMMARY_CONTAINER, SUMMARY_BLOB_NAME, readable_text)\n",
    "json_text = build_html_summary(\n",
    "  iso_week=iso_week,\n",
    "  start=start,\n",
    "  end=end,\n",
    "  window_df=window_df,\n",
    "  summary_df=summary_df,\n",
    "  top_k=10)\n",
    "send_email(json_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
